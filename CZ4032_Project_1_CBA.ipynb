{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CZ4032 Project 1 - CBA",
      "provenance": [],
      "collapsed_sections": [
        "14N32Fq_2g6D",
        "SWh7txWI2Sjc",
        "amnCo0KT3gyE",
        "IwoOou5oQCSR",
        "X8Pcx1rOXxLg",
        "US5qQZ0FSzjd"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdmf5o5VbBQd"
      },
      "source": [
        "# **CZ4032 Project 1: Classification Based on Associations**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MGWUYnJaSjg"
      },
      "source": [
        "# Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtPrJuSeeZ6J"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "import functools\n",
        "import sys\n",
        "import math\n",
        "import csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14N32Fq_2g6D"
      },
      "source": [
        "## Functions to read from .data and .names files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1l0aoJNyPKx"
      },
      "source": [
        "1. .names has the attribute names and types (numerical, categorical and the class label)\n",
        "2. .data file has the actual raw data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHkcF-tHw9rk"
      },
      "source": [
        "\n",
        "# Read dataset and convert into a list.\n",
        "# path: directory of *.data file.\n",
        "def read_data(path):\n",
        "    data = []\n",
        "    with open(path, 'r') as csv_file:\n",
        "        reader = csv.reader(csv_file, delimiter=',')\n",
        "        for line in reader:\n",
        "            data.append(line)\n",
        "        while [] in data:\n",
        "            data.remove([])\n",
        "    return data\n",
        "\n",
        "\n",
        "# Read scheme file *.names and write down attributes and value types.\n",
        "# path: directory of *.names file.\n",
        "def read_scheme(path):\n",
        "    with open(path, 'r') as csv_file:\n",
        "        reader = csv.reader(csv_file, delimiter=',')\n",
        "        attributes = next(reader)\n",
        "        value_type = next(reader)\n",
        "    return attributes, value_type\n",
        "\n",
        "\n",
        "# convert string-type value into float-type.\n",
        "# data: data list returned by read_data.\n",
        "# value_type: list returned by read_scheme.\n",
        "def str2numerical(data, value_type):\n",
        "    #ct = 0\n",
        "    #print(\"str2numerical\")\n",
        "    size = len(data) \n",
        "    columns = len(data[0])\n",
        "    for i in range(size):\n",
        "        #ct = 0\n",
        "        for j in range(columns - 1):\n",
        "            if value_type[j] == 'numerical' and data[i][j] != '?':\n",
        "                #ct += 1\n",
        "                data[i][j] = float(data[i][j])\n",
        "        #print(\"ct: \",ct)\n",
        "    return data\n",
        "\n",
        "\n",
        "# Main method in this file, to get data list after processing and scheme list.\n",
        "# data_path: tell where *.data file stores.\n",
        "# scheme_path: tell where *.names file stores.\n",
        "def read(data_path, scheme_path):\n",
        "    data = read_data(data_path)\n",
        "    attributes, value_type = read_scheme(scheme_path)\n",
        "    data = str2numerical(data, value_type)\n",
        "    return data, attributes, value_type"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWh7txWI2Sjc"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tja-M8jtyUNl"
      },
      "source": [
        "The following preprocessing functions are performed on every dataset before rule generation and classification, and is for the discretization of continuous attributes\n",
        "<br> Another important function performed here is the binning of data to set bin labels for each attribute"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwTqfwTGws6F"
      },
      "source": [
        "# A block to be split\n",
        "# It has 4 member:\n",
        "#   data: the data table with a column of continuous-valued attribute and a column of class label\n",
        "#   size: number of data case in this table\n",
        "#   number_of_classes: obviously, the number of class in this table\n",
        "#   entropy: entropy of dataset\n",
        "class Block:\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.size = len(data)\n",
        "        classes = set([x[1] for x in data])     # get distinct class labels in this table\n",
        "        self.number_of_classes = len(set(classes))\n",
        "        self.entropy = calculate_entropy(data)\n",
        "\n",
        "\n",
        "# Calculate the entropy of dataset\n",
        "# parameter data: the data table to be used\n",
        "def calculate_entropy(data):\n",
        "    number_of_data = len(data)\n",
        "    classes = set([x[1] for x in data])\n",
        "    class_count = dict([(label, 0) for label in classes])\n",
        "    for data_case in data:\n",
        "        class_count[data_case[1]] += 1      # count the number of data case of each class\n",
        "    entropy = 0\n",
        "    for c in classes:\n",
        "        p = class_count[c] / number_of_data\n",
        "        entropy -= p * math.log2(p)         # calculate information entropy by its formula, where the base is 2\n",
        "    return entropy\n",
        "\n",
        "\n",
        "# Compute Gain(A, T: S) mentioned in Dougherty, Kohavi & Sahami (1995), i.e. entropy gained by splitting original_block\n",
        "#   into left_block and right_block\n",
        "# original_block: the block before partition\n",
        "# left_block: the block split which its value below boundary\n",
        "# right_block: the block above boundary\n",
        "def entropy_gain(original_block, left_block, right_block):\n",
        "    gain = original_block.entropy - \\\n",
        "            ((left_block.size / original_block.size) * left_block.entropy +\n",
        "            (right_block.size / original_block.size) * right_block.entropy)\n",
        "    return gain\n",
        "\n",
        "\n",
        "# Get minimum entropy gain required for a split of original_block into 2 blocks \"left\" and \"right\", see Dougherty,\n",
        "#   Kohavi & Sahami (1995)\n",
        "# original_block: the block before partition\n",
        "# left_block: the block split which its value below boundary\n",
        "# right_block: the block above boundary\n",
        "def min_gain(original_block, left_block, right_block):\n",
        "    delta = math.log2(math.pow(3, original_block.number_of_classes) - 2) - \\\n",
        "            (original_block.number_of_classes * original_block.entropy -\n",
        "             left_block.number_of_classes * left_block.entropy -\n",
        "             right_block.number_of_classes * right_block.entropy)\n",
        "    gain_sup = math.log2(original_block.size - 1) / original_block.size + delta / original_block.size\n",
        "    return gain_sup\n",
        "\n",
        "\n",
        "# Identify the best acceptable value to split block\n",
        "# block: a block of dataset\n",
        "# Return value: a list of (boundary, entropy gain, left block, right block) or\n",
        "#   None when it's unnecessary to split\n",
        "def split(block):\n",
        "    candidates = [x[0] for x in block.data]     # candidates is a list of values can be picked up as boundary\n",
        "    candidates = list(set(candidates))          # get different values in table\n",
        "    candidates.sort()                           # sort ascending\n",
        "    candidates = candidates[1:]                 # discard smallest, because by definition no value is smaller\n",
        "\n",
        "    wall = []       # wall is a list storing final boundary\n",
        "    for value in candidates:\n",
        "        # split by value into 2 groups, below & above\n",
        "        left_data = []\n",
        "        right_data = []\n",
        "        for data_case in block.data:\n",
        "            if data_case[0] < value:\n",
        "                left_data.append(data_case)\n",
        "            else:\n",
        "                right_data.append(data_case)\n",
        "\n",
        "        left_block = Block(left_data)\n",
        "        right_block = Block(right_data)\n",
        "\n",
        "        gain = entropy_gain(block, left_block, right_block)\n",
        "        threshold = min_gain(block, left_block, right_block)\n",
        "\n",
        "        # minimum threshold is met, the value is an acceptable candidate\n",
        "        if gain >= threshold:\n",
        "            wall.append([value, gain, left_block, right_block])\n",
        "\n",
        "    if wall:    # has candidate\n",
        "        wall.sort(key=lambda wall: wall[1], reverse=True)   # sort descending by \"gain\"\n",
        "        return wall[0]      # return best candidate with max entropy gain\n",
        "    else:\n",
        "        return None         # no need to split\n",
        "\n",
        "\n",
        "# Top-down recursive partition of a data block, append boundary into \"walls\"\n",
        "# block: a data block\n",
        "def partition(block):\n",
        "    walls = []\n",
        "\n",
        "    # inner recursive function, accumulate the partitioning values\n",
        "    # sub_block: just a data block\n",
        "    def recursive_split(sub_block):\n",
        "        wall_returned = split(sub_block)        # binary partition, get bin boundary\n",
        "        if wall_returned:                       # still can be spilt\n",
        "            walls.append(wall_returned[0])      # record this partitioning value\n",
        "            recursive_split(wall_returned[2])   # recursively process left block\n",
        "            recursive_split(wall_returned[3])   # recursively split right block\n",
        "        else:\n",
        "            return                              # end of recursion\n",
        "\n",
        "    recursive_split(block)      # call inner function\n",
        "    walls.sort()                # sort boundaries descending\n",
        "    return walls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA8ZqOjFwws3"
      },
      "source": [
        "def get_mode(arr):\n",
        "    mode = []\n",
        "    arr_appear = dict((a, arr.count(a)) for a in arr)   # count appearance times of each key\n",
        "    if max(arr_appear.values()) == 1:       # if max time is 1\n",
        "        return      # no mode here\n",
        "    else:\n",
        "        for k, v in arr_appear.items():     # else, mode is the number which has max time\n",
        "            if v == max(arr_appear.values()):\n",
        "                mode.append(k)\n",
        "    return mode[0]  # return first number if has many modes\n",
        "\n",
        "\n",
        "# Fill missing values in column column_no, when missing values ration below 50%.\n",
        "# data: original data list\n",
        "# column_no: identify the column No. of that to be filled\n",
        "def fill_missing_values(data, column_no):\n",
        "    size = len(data)\n",
        "    column_data = [x[column_no] for x in data]      # get that column\n",
        "    while '?' in column_data:\n",
        "        column_data.remove('?')\n",
        "    mode = get_mode(column_data)\n",
        "    for i in range(size):\n",
        "        if data[i][column_no] == '?':\n",
        "            data[i][column_no] = mode              # fill in mode\n",
        "    return data\n",
        "\n",
        "\n",
        "# Get the list needed by rmep.py, just glue the data column with class column.\n",
        "# data_column: the data column\n",
        "# class_column: the class label column\n",
        "def get_discretization_data(data_column, class_column):\n",
        "    size = len(data_column)\n",
        "    result_list = []\n",
        "    for i in range(size):\n",
        "        result_list.append([data_column[i], class_column[i]])\n",
        "    return result_list\n",
        "\n",
        "\n",
        "# Replace numerical data with the No. of interval, i.e. consecutive positive integers.\n",
        "# data: original data table\n",
        "# column_no: the column No. of that column\n",
        "# walls: the split point of the whole range\n",
        "def replace_numerical(data, column_no, walls):\n",
        "    size = len(data)\n",
        "    num_spilt_point = len(walls)\n",
        "    for i in range(size):\n",
        "        if data[i][column_no] > walls[num_spilt_point - 1]:\n",
        "            data[i][column_no] = num_spilt_point + 1\n",
        "            continue\n",
        "        for j in range(0, num_spilt_point):\n",
        "            if data[i][column_no] <= walls[j]:\n",
        "                data[i][column_no] = j + 1\n",
        "                break\n",
        "    return data\n",
        "\n",
        "\n",
        "# Replace categorical values with a positive integer.\n",
        "# data: original data table\n",
        "# column_no: identify which column to be processed\n",
        "def replace_categorical(data, column_no):\n",
        "    size = len(data)\n",
        "    classes = set([x[column_no] for x in data])\n",
        "    classes_no = dict([(label, 0) for label in classes])\n",
        "    j = 1\n",
        "    for i in classes:\n",
        "        classes_no[i] = j\n",
        "        j += 1\n",
        "    for i in range(size):\n",
        "        data[i][column_no] = classes_no[data[i][column_no]]\n",
        "    return data, classes_no\n",
        "\n",
        "\n",
        "# Discard all the column with its column_no in discard_list\n",
        "# data: original data set\n",
        "# discard_list: a list of column No. of the columns to be discarded\n",
        "def discard(data, discard_list):\n",
        "    size = len(data)\n",
        "    length = len(data[0])\n",
        "    data_result = []\n",
        "    for i in range(size):\n",
        "        data_result.append([])\n",
        "        for j in range(length):\n",
        "            if j not in discard_list:\n",
        "                data_result[i].append(data[i][j])\n",
        "    return data_result\n",
        "\n",
        "\n",
        "# Main method here, see Description in detail\n",
        "# data: original data table\n",
        "# attribute: a list of the name of attribute\n",
        "# value_type: a list identifying the type of each column\n",
        "# Returned value: a data table after process\n",
        "def pre_process(data, attribute, value_type):\n",
        "    column_num = len(data[0])\n",
        "    size = len(data)\n",
        "    class_column = [x[-1] for x in data]\n",
        "    discard_list = []\n",
        "    for i in range(0, column_num - 1):\n",
        "        data_column = [x[i] for x in data]\n",
        "\n",
        "        # process missing values\n",
        "        missing_values_ratio = data_column.count('?') / size\n",
        "        if missing_values_ratio > 0.5:\n",
        "            discard_list.append(i)\n",
        "            continue\n",
        "        elif missing_values_ratio > 0:\n",
        "            data = fill_missing_values(data, i)\n",
        "            data_column = [x[i] for x in data]\n",
        "\n",
        "        # discretization\n",
        "        if value_type[i] == 'numerical':\n",
        "            discretization_data = get_discretization_data(data_column, class_column)\n",
        "            block = Block(discretization_data)\n",
        "            walls = partition(block)\n",
        "            if len(walls) == 0:\n",
        "                max_value = max(data_column)\n",
        "                min_value = min(data_column)\n",
        "                print(\"Attribute: \",attribute[i])\n",
        "                step = (max_value - min_value) / 3\n",
        "                walls.append(min_value + step)\n",
        "                walls.append(min_value + 2 * step)\n",
        "            #print(attribute[i] + \":\", walls)        # print out split points\n",
        "            data = replace_numerical(data, i, walls)\n",
        "        elif value_type[i] == 'categorical':\n",
        "            data, classes_no = replace_categorical(data, i)\n",
        "            print(attribute[i] + \":\", classes_no)   # print out replacement list\n",
        "\n",
        "    # discard\n",
        "    if len(discard_list) > 0:\n",
        "        data = discard(data, discard_list)\n",
        "        print(\"discard:\", discard_list)             # print out discard list\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amnCo0KT3gyE"
      },
      "source": [
        "# Rule Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7BLeam340yD"
      },
      "source": [
        "This preprocess function is specific to the rule generation step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H24NjD0HeHml"
      },
      "source": [
        "# Preprocesses a dataframe to a transaction list\n",
        "# Parameters:\n",
        "# [DataFrame] df = the pandas dataframe to preprocess\n",
        "# Returns:\n",
        "# [list] list of preprocessed transactions\n",
        "# [dict] dict of string to int replacements\n",
        "# [dict] dict of int to string replacements\n",
        "\n",
        "def preprocess_data(df):\n",
        "    values = set([np.nan])\n",
        "    for col in df.columns:\n",
        "        values = values.union(set(df[col].unique()))\n",
        "    replacement_dict = {k: v for v, k in enumerate(values)}\n",
        "    inverse_dict = dict(map(reversed, replacement_dict.items()))\n",
        "    preprocessed_df = df.replace(replacement_dict)\n",
        "    transactions = [[element for element in row if element != 0] for row in preprocessed_df.values.tolist()]\n",
        "\n",
        "    return transactions, replacement_dict, inverse_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsYl1weseKnc"
      },
      "source": [
        "# Postprocess to a dataframe from rules set\n",
        "# Parameters:\n",
        "# [set] rules = the rules set\n",
        "# Returns:\n",
        "# [DataFrame] returns the rules as a dataframe\n",
        "\n",
        "def postprocess_data(rules, inverse_dict):\n",
        "    import pandas as pd\n",
        "    df = pd.DataFrame(rules, columns=['LHS', 'RHS', 'Support', 'Confidence'])\n",
        "    df['RHS'] = df['RHS'].apply(lambda x: inverse_dict[x])\n",
        "    df['LHS'] = df['LHS'].apply(lambda x: [inverse_dict[y] for y in list(x)] )\n",
        "\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQ2R88dVeMbB"
      },
      "source": [
        "# Splits replacement dict values to ids and classes\n",
        "# Parameters:\n",
        "# [dict] replacement_dict = dict of string to int replacements\n",
        "# [list] classes_list = list of strings for classes\n",
        "# Returns:\n",
        "# [set] the ids for LHS\n",
        "# [set] the classes for RHS\n",
        "\n",
        "def split_classes_ids(replacement_dict, classes_list):\n",
        "    classes = set([replacement_dict[i] for i in classes_list])\n",
        "    ids = set(replacement_dict.values()) - classes\n",
        "    return ids, classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnpK-7odeRoR"
      },
      "source": [
        "class CARapriori:\n",
        "\n",
        "    def __init__(self, transactions):\n",
        "        self.transactions = transactions\n",
        "\n",
        "    # Performs the init pass\n",
        "    # Parameters:\n",
        "    # [list] transactions = The transaction list\n",
        "    # [set] ids = list of all occuring ids\n",
        "    # [set] target_ids = list of all class ids\n",
        "    # [float] min_support = Minimum specified support\n",
        "    # [float] min_confidence = Minimum specified confidence\n",
        "    # Returns:\n",
        "    # [dict] condsupCount_pruned = Returns a dict for all pruned candidates for counting occurences in transactions\n",
        "    # [dict] rulesupCount_pruned = Returns a dict for all pruned candidates for counting occurences together with target classes\n",
        "    # [set] target_ids = Set of all class ids\n",
        "    \n",
        "    def init_pass(self, ids, target_ids, min_support, min_confidence):\n",
        "        #print(\"Init_pass\")\n",
        "        candidates = self.car_candidate_gen(target_ids, ids)\n",
        "        condsupCount, rulesupCount = self.init_counters(candidates)\n",
        "        \n",
        "        condsupCount, rulesupCount = self.search(self.transactions, candidates, target_ids, condsupCount, rulesupCount)\n",
        "        print(condsupCount)\n",
        "        counters_rc_pruned, rulesupCount_pruned = self.prune(len(self.transactions), condsupCount, rulesupCount, min_support, min_confidence)\n",
        "        \n",
        "        return counters_rc_pruned, rulesupCount_pruned\n",
        "\n",
        "    # Used to initate counters to count support and confidence\n",
        "    # Parameters:\n",
        "    # [set] candidate_sets = Returns a set of all expanded test sets\n",
        "    # Returns:\n",
        "    # [dict] condsupCount = Returns an empty dict for all candidates for counting occurences in transactions\n",
        "    # [dict] rulesupCount = Returns an empty dict for all candidates for counting occurences together with target classes\n",
        "    \n",
        "    def init_counters(self, candidate_sets):\n",
        "        rulesupCount = {}\n",
        "        condsupCount = {}\n",
        "        for c in candidate_sets:\n",
        "            rulesupCount[c] = 0\n",
        "            condsupCount[c[0]] = 0\n",
        "            \n",
        "        return condsupCount, rulesupCount\n",
        "\n",
        "    # Used to generate new testable permutations\n",
        "    # Parameters:\n",
        "    # [list] target_ids = list of all class ids\n",
        "    # [set] f_k1 = set of tuples of items\n",
        "    # [set] c_condition_set = set of already explored items\n",
        "    # Returns:\n",
        "    # [list] c = List of candidates\n",
        "    \n",
        "    def car_candidate_gen(self, target_ids, f_k, c_condition_set = set()):\n",
        "        c = list()\n",
        "        for class_ in target_ids:\n",
        "            for item_ in f_k:\n",
        "                item_set = c_condition_set.copy()\n",
        "                if isinstance(item_, tuple):\n",
        "                    item_set.add(item_[0])\n",
        "                else:\n",
        "                    item_set.add(item_)\n",
        "                item_set = tuple(item_set)\n",
        "                c.append(tuple([item_set,class_]))\n",
        "        return c\n",
        "        \n",
        "    # Used to create new test sets\n",
        "    # Parameters:\n",
        "    # [set] last_pruned = the remaining item ids\n",
        "    # [set] target_ids = Set of all class ids\n",
        "    # Returns:\n",
        "    # [set] candidate_sets = Returns a set of all expanded test sets\n",
        "    \n",
        "    def expand(self, last_pruned, target_ids):\n",
        "        common_set = set()\n",
        "        \n",
        "        for key in last_pruned.keys():\n",
        "            common_set.add(key[0])\n",
        "        \n",
        "        candidate_sets = set()\n",
        "        for key in last_pruned.keys():\n",
        "            new_set = common_set.copy()\n",
        "            new_set.remove(key[0])\n",
        "            \n",
        "            candidates = self.car_candidate_gen(target_ids, new_set, set(key[0]))\n",
        "            candidate_sets = candidate_sets.union(candidates)\n",
        "        \n",
        "        return candidate_sets\n",
        "\n",
        "    # Search the transaction list and count occurences.\n",
        "    # Parameters:\n",
        "    # [list] transactions = The transaction list\n",
        "    # [set] candidate_sets = A set of all expanded test sets\n",
        "    # [set] target_ids = Set of all class ids\n",
        "    # [dict] condsupCount = An empty dict for all candidates for counting occurences in transactions\n",
        "    # [dict] rulesupCount = An empty dict for all candidates for counting occurences together with target classes\n",
        "    # Returns:\n",
        "    # [dict] condsupCount = Returns a dict for all candidates for counting occurences in transactions\n",
        "    # [dict] rulesupCount = Returns a dict for all candidates for counting occurences together with target classes\n",
        "    \n",
        "    def search(self, transactions, candidate_sets, target_ids, condsupCount, rulesupCount):\n",
        "        #print(\"Search begins, before outer loop\")\n",
        "        for t in transactions:\n",
        "            t_set = set(t)\n",
        "            classes_in_trans = t_set.intersection(target_ids)\n",
        "            found_in_transaction = {}\n",
        "            #print(\"Before inner loop\")\n",
        "            for c in candidate_sets:\n",
        "                #print(\"Inner loop\")\n",
        "                items_set = set(c[0])\n",
        "                items_in_trans = t_set.intersection(items_set)\n",
        "                \n",
        "                if items_in_trans == items_set:\n",
        "                    t_item_set = tuple(items_set)\n",
        "                    if t_item_set not in found_in_transaction:\n",
        "                        #print(t_item_set)\n",
        "                        #print(condsupCount)\n",
        "                        if condsupCount.get(t_item_set) == None:\n",
        "                            condsupCount[t_item_set] = 0\n",
        "                        condsupCount[t_item_set] += 1\n",
        "                        found_in_transaction[t_item_set] = True\n",
        "                    \n",
        "                    if c[1] in classes_in_trans:\n",
        "                        rulesupCount[tuple(c)] += 1\n",
        "            #print(condsupCount)\n",
        "                        \n",
        "        return condsupCount, rulesupCount\n",
        "\n",
        "    # Prunes the results based on the given counters and thresholds\n",
        "    # Parameters:\n",
        "    # [int] transactions_length = The transaction list length\n",
        "    # [dict] condsupCount = A dict for all candidates for counting occurences in transactions\n",
        "    # [dict] rulesupCount = A dict for all candidates for counting occurences together with target classes\n",
        "    # [float] min_support = Minimum specified support\n",
        "    # [float] min_confidence = Minimum specified confidence\n",
        "    # Returns:\n",
        "    # [dict] condsupCount_pruned = Returns a dict for all pruned candidates for counting occurences in transactions\n",
        "    # [dict] rulesupCount_pruned = Returns a dict for all pruned candidates for counting occurences together with target classes\n",
        "    \n",
        "    def prune(self, transactions_length, condsupCount, rulesupCount, min_support, min_confidence):\n",
        "        condsupCount_pruned = dict()\n",
        "        rulesupCount_pruned = dict()\n",
        "        \n",
        "        for key, val in condsupCount.items():\n",
        "            if val > 0:\n",
        "                support = round(val/transactions_length, 3)\n",
        "                if support >= min_support:\n",
        "                    condsupCount_pruned[key] = support\n",
        "            \n",
        "        for key, val in rulesupCount.items():\n",
        "            if val > 0 and key[0] in condsupCount_pruned:\n",
        "                confidence = round(val/condsupCount[key[0]], 3)\n",
        "                if confidence >= min_confidence:\n",
        "                    rulesupCount_pruned[key] = confidence\n",
        "        \n",
        "        return condsupCount_pruned, rulesupCount_pruned\n",
        "\n",
        "\n",
        "    # Add the rules to the set\n",
        "    # Parameters:\n",
        "    # [set] rules = New generated rules\n",
        "    # [dict] condsupCount_pruned = A dict for all pruned candidates for counting occurences in transactions\n",
        "    # [dict] rulesupCount_pruned = A dict for all pruned candidates for counting occurences together with target classes\n",
        "    # Returns:\n",
        "    # [bool] Returns True if new rules are added\n",
        "    \n",
        "    def add_rules(self, rules, counters_rc_pruned, rulesupCount_pruned):\n",
        "    \n",
        "        rules_before = len(rules)\n",
        "        for key, val in rulesupCount_pruned.items():\n",
        "            rules.add(tuple([key[0],key[1],counters_rc_pruned[key[0]],val]))   \n",
        "        rules_after = len(rules)\n",
        "        \n",
        "        #return True if new rules added\n",
        "        return rules_after > rules_before \n",
        "        \n",
        "    # Main function\n",
        "    # Parameters:\n",
        "    # [set] ids = list of all occuring ids\n",
        "    # [set] target_ids = list of all class ids\n",
        "    # [float] min_support = Minimum specified support\n",
        "    # [float] min_confidence = Minimum specified confidence\n",
        "    # [int] max_length = Maximum rule length to search for\n",
        "    # Returns:\n",
        "    # [set] Returns the data mined rules\n",
        "    \n",
        "    def run(self, ids, target_ids, min_support=0.01, min_confidence=0.5, max_length=2):\n",
        "        rules = set()\n",
        "        \n",
        "        #print(\"inital pass\")\n",
        "        counters_rc_pruned, rulesupCount_pruned = self.init_pass(ids, target_ids, min_support, min_confidence)\n",
        "        #print(\"Counter: \",counters_rc_pruned)\n",
        "        #print(\"RulesupCount: \",rulesupCount_pruned)\n",
        "\n",
        "        #try to add new rules\n",
        "        rules_added = self.add_rules(rules, counters_rc_pruned, rulesupCount_pruned)\n",
        "\n",
        "        #print(\"rules_added\",rules_added)\n",
        "        if rules_added:\n",
        "            for iteration in range(max_length):\n",
        "\n",
        "                #print(\"expand new candidates\")\n",
        "                candidate_sets = self.expand(rulesupCount_pruned, target_ids)\n",
        "                #print(\"init counters\")\n",
        "                counters_rc, rulesupCount = self.init_counters(candidate_sets)\n",
        "                #print(counters_rc)\n",
        "                #search for test sets\n",
        "                counters_rc, rulesupCount = self.search(self.transactions, candidate_sets, target_ids, counters_rc, rulesupCount)\n",
        "                #prune\n",
        "                counters_rc_pruned, rulesupCount_pruned = self.prune(len(self.transactions), counters_rc, rulesupCount, min_support, min_confidence)\n",
        "                #add\n",
        "                rules_added = self.add_rules(rules, counters_rc_pruned, rulesupCount_pruned)\n",
        "\n",
        "                if not rules_added:\n",
        "                #early stopping\n",
        "                    break\n",
        "\n",
        "\n",
        "        return rules\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs0R5s9dyZ-5"
      },
      "source": [
        "Read data from data and names files using Read function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PIizYdLW-Uu"
      },
      "source": [
        "data_path = 'iris.data' # only change filename here for different datasets\n",
        "scheme_path = 'iris.names'\n",
        "data, attributes, value_type = read(data_path, scheme_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ksvf85TqehCI",
        "outputId": "18a3ff0d-51bf-4ee8-dbf7-3a187fc79937"
      },
      "source": [
        "#random.shuffle(data)\n",
        "d = pre_process(data, attributes, value_type)\n",
        "#print(type(df))\n",
        "print(d)\n",
        "#df = data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 3, 1, 1, 'Iris-setosa'], [1, 1, 1, 1, 'Iris-setosa'], [1, 2, 1, 1, 'Iris-setosa'], [1, 2, 1, 1, 'Iris-setosa'], [1, 3, 1, 1, 'Iris-setosa'], [1, 3, 1, 1, 'Iris-setosa'], [1, 2, 1, 1, 'Iris-setosa'], [1, 2, 1, 1, 'Iris-setosa'], [1, 1, 1, 1, 'Iris-setosa'], [1, 2, 1, 1, 'Iris-setosa'], [1, 3, 1, 1, 'Iris-setosa'], [1, 2, 1, 1, 'Iris-setosa'], [1, 1, 1, 1, 'Iris-setosa'], [1, 1, 1, 1, 'Iris-setosa'], [2, 3, 1, 1, 'Iris-setosa'], [2, 3, 1, 1, 'Iris-setosa'], [1, 3, 1, 1, 'Iris-setosa'], [1, 3, 1, 1, 'Iris-setosa'], [2, 3, 1, 1, 'Iris-setosa'], [1, 3, 1, 1, 'Iris-setosa'], [1, 2, 1, 1, 'Iris-setosa'], [1, 3, 1, 1, 'Iris-setosa'], [1, 3, 1, 1, 'Iris-setosa'], [1, 2, 1, 1, 'Iris-setosa'], [1, 2, 1, 1, 'Iris-setosa'], [1, 1, 1, 1, 'Iris-setosa'], [1, 2, 1, 1, 'Iris-setosa'], [1, 3, 1, 1, 'Iris-setosa'], [1, 2, 1, 1, 'Iris-setosa'], [1, 2, 1, 1, 'Iris-setosa'], [1, 2, 1, 1, 'Iris-setosa'], [1, 2, 1, 1, 'Iris-setosa'], [1, 3, 1, 1, 'Iris-setosa'], [1, 3, 1, 1, 'Iris-setosa'], [1, 2, 1, 1, 'Iris-setosa'], [1, 2, 1, 1, 'Iris-setosa'], [1, 3, 1, 1, 'Iris-setosa'], [1, 2, 1, 1, 'Iris-setosa'], [1, 1, 1, 1, 'Iris-setosa'], [1, 2, 1, 1, 'Iris-setosa'], [1, 3, 1, 1, 'Iris-setosa'], [1, 1, 1, 1, 'Iris-setosa'], [1, 2, 1, 1, 'Iris-setosa'], [1, 3, 1, 1, 'Iris-setosa'], [1, 3, 1, 1, 'Iris-setosa'], [1, 1, 1, 1, 'Iris-setosa'], [1, 3, 1, 1, 'Iris-setosa'], [1, 2, 1, 1, 'Iris-setosa'], [1, 3, 1, 1, 'Iris-setosa'], [1, 2, 1, 1, 'Iris-setosa'], [3, 2, 2, 2, 'Iris-versicolor'], [3, 2, 2, 2, 'Iris-versicolor'], [3, 2, 3, 2, 'Iris-versicolor'], [1, 1, 2, 2, 'Iris-versicolor'], [3, 1, 2, 2, 'Iris-versicolor'], [2, 1, 2, 2, 'Iris-versicolor'], [3, 2, 2, 2, 'Iris-versicolor'], [1, 1, 2, 1, 'Iris-versicolor'], [3, 1, 2, 2, 'Iris-versicolor'], [1, 1, 2, 2, 'Iris-versicolor'], [1, 1, 2, 1, 'Iris-versicolor'], [2, 1, 2, 2, 'Iris-versicolor'], [2, 1, 2, 1, 'Iris-versicolor'], [2, 1, 2, 2, 'Iris-versicolor'], [1, 1, 2, 2, 'Iris-versicolor'], [3, 2, 2, 2, 'Iris-versicolor'], [1, 1, 2, 2, 'Iris-versicolor'], [2, 1, 2, 1, 'Iris-versicolor'], [2, 1, 2, 2, 'Iris-versicolor'], [1, 1, 2, 2, 'Iris-versicolor'], [2, 2, 2, 2, 'Iris-versicolor'], [2, 1, 2, 2, 'Iris-versicolor'], [3, 1, 3, 2, 'Iris-versicolor'], [2, 1, 2, 2, 'Iris-versicolor'], [3, 1, 2, 2, 'Iris-versicolor'], [3, 1, 2, 2, 'Iris-versicolor'], [3, 1, 2, 2, 'Iris-versicolor'], [3, 1, 3, 2, 'Iris-versicolor'], [2, 1, 2, 2, 'Iris-versicolor'], [2, 1, 2, 1, 'Iris-versicolor'], [1, 1, 2, 2, 'Iris-versicolor'], [1, 1, 2, 1, 'Iris-versicolor'], [2, 1, 2, 2, 'Iris-versicolor'], [2, 1, 3, 2, 'Iris-versicolor'], [1, 1, 2, 2, 'Iris-versicolor'], [2, 2, 2, 2, 'Iris-versicolor'], [3, 2, 2, 2, 'Iris-versicolor'], [3, 1, 2, 2, 'Iris-versicolor'], [1, 1, 2, 2, 'Iris-versicolor'], [1, 1, 2, 2, 'Iris-versicolor'], [1, 1, 2, 2, 'Iris-versicolor'], [2, 1, 2, 2, 'Iris-versicolor'], [2, 1, 2, 2, 'Iris-versicolor'], [1, 1, 2, 1, 'Iris-versicolor'], [1, 1, 2, 2, 'Iris-versicolor'], [2, 1, 2, 2, 'Iris-versicolor'], [2, 1, 2, 2, 'Iris-versicolor'], [2, 1, 2, 2, 'Iris-versicolor'], [1, 1, 1, 2, 'Iris-versicolor'], [2, 1, 2, 2, 'Iris-versicolor'], [3, 2, 3, 3, 'Iris-virginica'], [2, 1, 3, 3, 'Iris-virginica'], [3, 1, 3, 3, 'Iris-virginica'], [3, 1, 3, 2, 'Iris-virginica'], [3, 1, 3, 3, 'Iris-virginica'], [3, 1, 3, 3, 'Iris-virginica'], [1, 1, 2, 2, 'Iris-virginica'], [3, 1, 3, 2, 'Iris-virginica'], [3, 1, 3, 2, 'Iris-virginica'], [3, 3, 3, 3, 'Iris-virginica'], [3, 2, 3, 3, 'Iris-virginica'], [3, 1, 3, 3, 'Iris-virginica'], [3, 1, 3, 3, 'Iris-virginica'], [2, 1, 3, 3, 'Iris-virginica'], [2, 1, 3, 3, 'Iris-virginica'], [3, 2, 3, 3, 'Iris-virginica'], [3, 1, 3, 2, 'Iris-virginica'], [3, 3, 3, 3, 'Iris-virginica'], [3, 1, 3, 3, 'Iris-virginica'], [2, 1, 3, 2, 'Iris-virginica'], [3, 2, 3, 3, 'Iris-virginica'], [1, 1, 3, 3, 'Iris-virginica'], [3, 1, 3, 3, 'Iris-virginica'], [3, 1, 3, 2, 'Iris-virginica'], [3, 2, 3, 3, 'Iris-virginica'], [3, 2, 3, 2, 'Iris-virginica'], [2, 1, 2, 2, 'Iris-virginica'], [2, 1, 3, 2, 'Iris-virginica'], [3, 1, 3, 3, 'Iris-virginica'], [3, 1, 3, 2, 'Iris-virginica'], [3, 1, 3, 3, 'Iris-virginica'], [3, 3, 3, 3, 'Iris-virginica'], [3, 1, 3, 3, 'Iris-virginica'], [3, 1, 3, 2, 'Iris-virginica'], [2, 1, 3, 2, 'Iris-virginica'], [3, 1, 3, 3, 'Iris-virginica'], [3, 2, 3, 3, 'Iris-virginica'], [3, 2, 3, 2, 'Iris-virginica'], [2, 1, 2, 2, 'Iris-virginica'], [3, 2, 3, 3, 'Iris-virginica'], [3, 2, 3, 3, 'Iris-virginica'], [3, 2, 3, 3, 'Iris-virginica'], [2, 1, 3, 3, 'Iris-virginica'], [3, 2, 3, 3, 'Iris-virginica'], [3, 2, 3, 3, 'Iris-virginica'], [3, 1, 3, 3, 'Iris-virginica'], [3, 1, 3, 3, 'Iris-virginica'], [3, 1, 3, 3, 'Iris-virginica'], [2, 2, 3, 3, 'Iris-virginica'], [2, 1, 3, 2, 'Iris-virginica']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqLcUiTkyf4U"
      },
      "source": [
        "Convert values of list from <value.> to <column no.,value>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ld8T6we4xS_0"
      },
      "source": [
        "for i in range(len(d)):\n",
        "  for j in range(len(d[i]) - 1):\n",
        "    #print(df[i][j])\n",
        "    d[i][j] = str(\"(\") + str(j) + \",\" + str(d[i][j]) + str(\")\") #this has to be a datatype acceptable by pandas unique\n",
        "    #df[i][j] = tuple(str(j)) + tuple(str(df[i][j]))\n",
        "    #print(df[i][j])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9FdpbZcxFo8"
      },
      "source": [
        "d = pd.DataFrame(d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwFg6l-wHFrX"
      },
      "source": [
        "df = d.sample(frac=0.7,random_state = 25)\n",
        "test_df = d.drop(df.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjsaDKI5gsXL"
      },
      "source": [
        "d = d.values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UhJRtLjb3OP"
      },
      "source": [
        "test_df = test_df.values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFnz1wGmy2O1",
        "outputId": "3e231fbd-9822-42a3-f825-a3884698dd26"
      },
      "source": [
        "classLabels = pd.unique(df[len(df.columns)-1])\n",
        "print((classLabels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Iris-setosa' 'Iris-versicolor' 'Iris-virginica']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QJhLebifHze",
        "outputId": "83283366-5786-443f-dbf6-a2802c77698d"
      },
      "source": [
        "transactions, replacement_dict, inverse_dict = preprocess_data(df)\n",
        "# print(\"Transactions: \",transactions)\n",
        "# print(\"Replacement_dict: \", replacement_dict)\n",
        "# print(\"Inverse_dict: \", inverse_dict)\n",
        "car = CARapriori(transactions)\n",
        "# print(\"Car: \",car)\n",
        "ids, classes = split_classes_ids(replacement_dict, classLabels)\n",
        "# print(\"Id: \",ids)\n",
        "# print(\"Classes: \",classes)\n",
        "\n",
        "minsup = 0.01\n",
        "minconf = 0.5\n",
        "rules = car.run(ids, classes,minsup,minconf,3)\n",
        "print('CARs')\n",
        "#print(rules)\n",
        "final = postprocess_data(rules, inverse_dict)\n",
        "print(final)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{(0,): 0, (1,): 45, (2,): 35, (3,): 17, (4,): 25, (5,): 44, (8,): 40, (10,): 35, (11,): 57, (12,): 34, (13,): 31, (14,): 36, (15,): 21}\n",
            "CARs\n",
            "                              LHS              RHS  Support  Confidence\n",
            "0                  [(1,1), (3,3)]   Iris-virginica    0.114       1.000\n",
            "1    [(0,1), (2,2), (1,1), (3,2)]  Iris-versicolor    0.076       1.000\n",
            "2           [(0,3), (2,3), (3,3)]   Iris-virginica    0.152       1.000\n",
            "3                  [(1,3), (3,3)]   Iris-virginica    0.010       1.000\n",
            "4           [(0,3), (1,1), (3,2)]  Iris-versicolor    0.124       0.615\n",
            "..                            ...              ...      ...         ...\n",
            "108                [(0,2), (3,2)]  Iris-versicolor    0.162       0.765\n",
            "109                [(0,3), (1,1)]   Iris-virginica    0.200       0.619\n",
            "110  [(3,1), (0,1), (1,3), (2,1)]      Iris-setosa    0.133       1.000\n",
            "111                [(2,2), (1,1)]  Iris-versicolor    0.286       0.933\n",
            "112         [(2,2), (0,3), (3,2)]  Iris-versicolor    0.086       1.000\n",
            "\n",
            "[113 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D0tKe98C4N-"
      },
      "source": [
        "Preparing Dataset and Rules for comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzWcpppLeudN"
      },
      "source": [
        "cars = final.values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQyckOA3fO-I"
      },
      "source": [
        "df = df.values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aI26SG3lLbKI"
      },
      "source": [
        "# Classification\n",
        "This section has all relevant functions and classes for our classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2HKu5FmNoDd"
      },
      "source": [
        "is_satisfy is the main check function which compares each datacase to each generated rule"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enzuTHyFtaUt"
      },
      "source": [
        "def is_satisfy(datacase,rulecase):\n",
        "  #print(\"Datacase: \",datacase[:-2])\n",
        "  #print(\"Rulecase: \",rulecase[0])\n",
        "  result =  all(elem in datacase[:-1] for elem in rulecase[0])\n",
        "  #print(\"result: \",result)\n",
        "  if not result:\n",
        "  #if datacase[:-2] != rulecase[0] or datacase[-1] != rulecase[1]: #if datacase elements do not match condset, return none\n",
        "    return None\n",
        "  elif datacase[-1] == rulecase[1]: #if condset & class label match\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSx23awBXLak"
      },
      "source": [
        "### CBA-CB  - M1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HYWJZXiIzoh"
      },
      "source": [
        "class Classifier:\n",
        "    \"\"\"\n",
        "    This class is our classifier.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.rule_list = list()\n",
        "        self.default_class = None\n",
        "        self._error_list = list()\n",
        "        self._default_class_list = list()\n",
        "\n",
        "    # insert a rule into rule_list, then choose a default class, and calculate the errors (see line 8, 10 & 11)\n",
        "    def insert(self, rule, dataset):\n",
        "        #print(\"Insert\")\n",
        "        self.rule_list.append(rule)             # insert r at the end of C\n",
        "        self.sel_defclass(dataset)     # select a default class for the current C\n",
        "        self.comp_err(dataset)            # compute the total number of errors of C\n",
        "\n",
        "    # select the majority class in the remaining data\n",
        "    def sel_defclass(self, dataset):\n",
        "        #print(\"Select default class\")\n",
        "        class_column = [x[-1] for x in dataset]\n",
        "        class_label = set(class_column)\n",
        "        max = 0\n",
        "        current_default_class = None\n",
        "        for label in class_label:\n",
        "            if class_column.count(label) >= max:\n",
        "                max = class_column.count(label)\n",
        "                current_default_class = label\n",
        "        self._default_class_list.append(current_default_class)\n",
        "\n",
        "    # compute the total number of errors\n",
        "    def comp_err(self, dataset):\n",
        "        #print(\"Compute error\")\n",
        "        if len(dataset) <= 0:\n",
        "            self._error_list.append(sys.maxsize)\n",
        "            return\n",
        "\n",
        "        error_number = 0\n",
        "\n",
        "        # the total number of errors that have been made by all the selected rules in C\n",
        "        for case in dataset:\n",
        "            is_cover = False\n",
        "            for rule in self.rule_list:\n",
        "                if is_satisfy(case, rule):\n",
        "                    is_cover = True\n",
        "                    break\n",
        "            if not is_cover:\n",
        "                error_number += 1\n",
        "\n",
        "        # the number of errors to be made by the default class in the training set\n",
        "        class_column = [x[-1] for x in dataset]\n",
        "        error_number += len(class_column) - class_column.count(self._default_class_list[-1])\n",
        "        self._error_list.append(error_number)\n",
        "\n",
        "    # see line 14 and 15, to get the final classifier\n",
        "    def discard(self):\n",
        "        #print(\"discard\")\n",
        "        # find the first rule p in C with the lowest total number of errors and drop all the rules after p in C\n",
        "        index = self._error_list.index(min(self._error_list))\n",
        "        self.rule_list = self.rule_list[:(index+1)]\n",
        "        self._error_list = None\n",
        "\n",
        "        # assign the default class associated with p to default_class\n",
        "        self.default_class = self._default_class_list[index]\n",
        "        self._default_class_list = None\n",
        "\n",
        "    # just print out all selected rules and default class in our classifier\n",
        "    def print(self):\n",
        "        print(\"Selected Rules: \") \n",
        "        for rule in self.rule_list:\n",
        "            print(rule)\n",
        "        print(\"default_class:\", self.default_class)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH2Q3XgdPqPT"
      },
      "source": [
        "Precedence calculation is a key step for our classification because our rule list is sorted according to the precedence of the rules and this helps us to quickly cover each training case with the most apt rule"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dR0MS3NFsNzb"
      },
      "source": [
        "# sort the set of generated rules car according to the precedence and return the sorted rule list\n",
        "def prec_sort(car):\n",
        "    def cmp_method(a, b):\n",
        "        if a[3] < b[3]:     # 1. the confidence of ri > rj\n",
        "            return 1\n",
        "        elif a[3] == b[3]:\n",
        "            if a[2] < b[2]:       # 2. their confidences are the same, but support of ri > rj\n",
        "                return 1\n",
        "            elif a[2] == b[2]:\n",
        "                if len(a[0]) < len(b[0]):   # 3. both confidence & support are the same, ri earlier than rj\n",
        "                    return -1\n",
        "                elif len(a[0]) == len(b[0]):\n",
        "                    return 0\n",
        "                else:\n",
        "                    return 1\n",
        "            else:\n",
        "                return -1\n",
        "        else:\n",
        "            return -1\n",
        "\n",
        "    rule_list = car\n",
        "    rule_list.sort(key=functools.cmp_to_key(cmp_method))\n",
        "    # for a,b in itertools.combinations_with_replacement(rule_list,2):\n",
        "    #   while(rule_list.index(a)!=rule_list.index(a)):\n",
        "    #     cmp_method(a,b)\n",
        "    return rule_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwoOou5oQCSR"
      },
      "source": [
        "#### CBA - M1 main classifier builder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7HZ_WNBsQV4"
      },
      "source": [
        "# main method of CBA-CB: M1\n",
        "def classifier_builder_m1(cars, dataset):\n",
        "    #print(\"Entered builder\")\n",
        "    classifier = Classifier()\n",
        "    #print(\"after Classifier()\")\n",
        "    cars_list = prec_sort(cars)\n",
        "    print(cars_list)\n",
        "    for rule in cars_list:\n",
        "        #print(\"inside loop\")\n",
        "        temp = []\n",
        "        mark = False\n",
        "        for i in range(len(dataset)):\n",
        "            #print(\"inside inner loop\")\n",
        "            is_satisfy_value = is_satisfy(dataset[i], rule)\n",
        "            #print(\"is_satisfy_value: \",is_satisfy_value)\n",
        "            if is_satisfy_value is not None:\n",
        "                temp.append(i)\n",
        "                if is_satisfy_value:\n",
        "                    mark = True\n",
        "        #print(\"mark: \",mark,\"dataset: \",dataset[i])\n",
        "        if mark:\n",
        "            temp_dataset = list(dataset)\n",
        "            for index in temp:\n",
        "                temp_dataset[index] = []\n",
        "            while [] in temp_dataset:\n",
        "                temp_dataset.remove([])\n",
        "            dataset = temp_dataset\n",
        "            classifier.insert(rule, dataset)\n",
        "            #print(\"inserted\")\n",
        "    classifier.discard()\n",
        "    return classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4Vg6sgXI2Ot",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "334a9639-d69b-4c39-9328-5ba656aca242"
      },
      "source": [
        "classifier_m1 = classifier_builder_m1(cars, df)\n",
        "classifier_m1.print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[['(2,1)'], 'Iris-setosa', 0.343, 1.0], [['(3,1)', '(2,1)'], 'Iris-setosa', 0.343, 1.0], [['(0,1)', '(2,1)'], 'Iris-setosa', 0.324, 1.0], [['(3,1)', '(0,1)', '(2,1)'], 'Iris-setosa', 0.324, 1.0], [['(3,3)'], 'Iris-virginica', 0.2, 1.0], [['(2,3)', '(3,3)'], 'Iris-virginica', 0.2, 1.0], [['(0,3)', '(3,3)'], 'Iris-virginica', 0.152, 1.0], [['(3,1)', '(1,3)'], 'Iris-setosa', 0.152, 1.0], [['(1,3)', '(2,1)'], 'Iris-setosa', 0.152, 1.0], [['(0,3)', '(2,3)', '(3,3)'], 'Iris-virginica', 0.152, 1.0], [['(3,1)', '(1,3)', '(2,1)'], 'Iris-setosa', 0.152, 1.0], [['(0,1)', '(1,3)'], 'Iris-setosa', 0.133, 1.0], [['(0,1)', '(1,3)', '(2,1)'], 'Iris-setosa', 0.133, 1.0], [['(3,1)', '(0,1)', '(1,3)'], 'Iris-setosa', 0.133, 1.0], [['(3,1)', '(0,1)', '(1,3)', '(2,1)'], 'Iris-setosa', 0.133, 1.0], [['(1,1)', '(3,3)'], 'Iris-virginica', 0.114, 1.0], [['(1,1)', '(2,3)', '(3,3)'], 'Iris-virginica', 0.114, 1.0], [['(0,1)', '(2,2)'], 'Iris-versicolor', 0.095, 1.0], [['(0,1)', '(2,2)', '(1,1)'], 'Iris-versicolor', 0.095, 1.0], [['(0,3)', '(2,2)'], 'Iris-versicolor', 0.086, 1.0], [['(2,2)', '(0,3)'], 'Iris-versicolor', 0.086, 1.0], [['(0,3)', '(2,2)', '(3,2)'], 'Iris-versicolor', 0.086, 1.0], [['(2,2)', '(0,3)', '(3,2)'], 'Iris-versicolor', 0.086, 1.0], [['(0,1)', '(3,2)'], 'Iris-versicolor', 0.076, 1.0], [['(0,1)', '(1,1)', '(3,2)'], 'Iris-versicolor', 0.076, 1.0], [['(0,1)', '(2,2)', '(3,2)'], 'Iris-versicolor', 0.076, 1.0], [['(0,3)', '(1,1)', '(3,3)'], 'Iris-virginica', 0.076, 1.0], [['(0,1)', '(2,2)', '(1,1)', '(3,2)'], 'Iris-versicolor', 0.076, 1.0], [['(0,3)', '(1,1)', '(2,3)', '(3,3)'], 'Iris-virginica', 0.076, 1.0], [['(1,1)', '(2,2)', '(0,3)'], 'Iris-versicolor', 0.057, 1.0], [['(0,3)', '(2,2)', '(1,1)'], 'Iris-versicolor', 0.057, 1.0], [['(1,1)', '(0,3)', '(2,2)'], 'Iris-versicolor', 0.057, 1.0], [['(2,2)', '(0,3)', '(1,1)'], 'Iris-versicolor', 0.057, 1.0], [['(1,1)', '(0,3)', '(2,2)', '(3,2)'], 'Iris-versicolor', 0.057, 1.0], [['(2,2)', '(0,3)', '(1,1)', '(3,2)'], 'Iris-versicolor', 0.057, 1.0], [['(0,3)', '(2,2)', '(1,1)', '(3,2)'], 'Iris-versicolor', 0.057, 1.0], [['(1,1)', '(2,2)', '(0,3)', '(3,2)'], 'Iris-versicolor', 0.057, 1.0], [['(1,1)', '(2,1)'], 'Iris-setosa', 0.048, 1.0], [['(3,1)', '(1,1)', '(2,1)'], 'Iris-setosa', 0.048, 1.0], [['(0,1)', '(1,1)', '(2,1)'], 'Iris-setosa', 0.048, 1.0], [['(3,1)', '(0,1)', '(1,1)', '(2,1)'], 'Iris-setosa', 0.048, 1.0], [['(0,2)', '(3,3)'], 'Iris-virginica', 0.038, 1.0], [['(3,1)', '(2,2)'], 'Iris-versicolor', 0.038, 1.0], [['(3,1)', '(2,2)', '(1,1)'], 'Iris-versicolor', 0.038, 1.0], [['(2,3)', '(0,2)', '(3,3)'], 'Iris-virginica', 0.038, 1.0], [['(0,2)', '(2,3)', '(3,3)'], 'Iris-virginica', 0.038, 1.0], [['(1,1)', '(0,2)', '(3,3)'], 'Iris-virginica', 0.029, 1.0], [['(1,1)', '(2,3)', '(0,2)', '(3,3)'], 'Iris-virginica', 0.029, 1.0], [['(1,1)', '(0,2)', '(2,3)', '(3,3)'], 'Iris-virginica', 0.029, 1.0], [['(0,2)', '(2,1)'], 'Iris-setosa', 0.019, 1.0], [['(1,3)', '(0,2)'], 'Iris-setosa', 0.019, 1.0], [['(3,1)', '(1,3)', '(0,2)'], 'Iris-setosa', 0.019, 1.0], [['(3,1)', '(1,1)', '(0,2)'], 'Iris-versicolor', 0.019, 1.0], [['(3,1)', '(0,1)', '(2,2)'], 'Iris-versicolor', 0.019, 1.0], [['(1,3)', '(0,2)', '(2,1)'], 'Iris-setosa', 0.019, 1.0], [['(3,1)', '(2,2)', '(0,2)'], 'Iris-versicolor', 0.019, 1.0], [['(3,1)', '(0,2)', '(2,1)'], 'Iris-setosa', 0.019, 1.0], [['(3,1)', '(2,2)', '(1,1)', '(0,2)'], 'Iris-versicolor', 0.019, 1.0], [['(3,1)', '(1,3)', '(0,2)', '(2,1)'], 'Iris-setosa', 0.019, 1.0], [['(3,1)', '(0,1)', '(2,2)', '(1,1)'], 'Iris-versicolor', 0.019, 1.0], [['(1,3)', '(3,3)'], 'Iris-virginica', 0.01, 1.0], [['(0,1)', '(2,3)'], 'Iris-virginica', 0.01, 1.0], [['(0,3)', '(1,3)'], 'Iris-virginica', 0.01, 1.0], [['(0,1)', '(3,3)'], 'Iris-virginica', 0.01, 1.0], [['(1,3)', '(2,3)'], 'Iris-virginica', 0.01, 1.0], [['(0,3)', '(1,3)', '(3,3)'], 'Iris-virginica', 0.01, 1.0], [['(0,1)', '(1,1)', '(2,3)'], 'Iris-virginica', 0.01, 1.0], [['(0,1)', '(1,1)', '(3,3)'], 'Iris-virginica', 0.01, 1.0], [['(0,1)', '(2,3)', '(3,3)'], 'Iris-virginica', 0.01, 1.0], [['(1,3)', '(2,3)', '(3,3)'], 'Iris-virginica', 0.01, 1.0], [['(0,3)', '(1,3)', '(2,3)'], 'Iris-virginica', 0.01, 1.0], [['(0,3)', '(1,3)', '(2,3)', '(3,3)'], 'Iris-virginica', 0.01, 1.0], [['(0,1)', '(1,1)', '(2,3)', '(3,3)'], 'Iris-virginica', 0.01, 1.0], [['(3,1)', '(0,1)'], 'Iris-setosa', 0.343, 0.944], [['(2,2)'], 'Iris-versicolor', 0.333, 0.943], [['(1,3)'], 'Iris-setosa', 0.162, 0.941], [['(2,2)', '(3,2)'], 'Iris-versicolor', 0.295, 0.935], [['(2,2)', '(1,1)'], 'Iris-versicolor', 0.286, 0.933], [['(2,2)', '(1,1)', '(3,2)'], 'Iris-versicolor', 0.248, 0.923], [['(3,1)'], 'Iris-setosa', 0.381, 0.9], [['(0,3)', '(2,3)'], 'Iris-virginica', 0.248, 0.885], [['(2,3)'], 'Iris-virginica', 0.324, 0.882], [['(2,2)', '(0,2)'], 'Iris-versicolor', 0.152, 0.875], [['(0,3)', '(1,1)', '(2,3)'], 'Iris-virginica', 0.143, 0.867], [['(1,1)', '(2,3)'], 'Iris-virginica', 0.21, 0.864], [['(2,2)', '(0,2)', '(3,2)'], 'Iris-versicolor', 0.133, 0.857], [['(2,2)', '(1,1)', '(0,2)'], 'Iris-versicolor', 0.133, 0.857], [['(0,2)', '(2,3)'], 'Iris-virginica', 0.067, 0.857], [['(2,3)', '(0,2)'], 'Iris-virginica', 0.067, 0.857], [['(2,2)', '(1,1)', '(0,2)', '(3,2)'], 'Iris-versicolor', 0.114, 0.833], [['(1,1)', '(2,3)', '(0,2)'], 'Iris-virginica', 0.057, 0.833], [['(1,1)', '(0,2)', '(2,3)'], 'Iris-virginica', 0.057, 0.833], [['(0,2)', '(3,2)'], 'Iris-versicolor', 0.162, 0.765], [['(0,1)'], 'Iris-setosa', 0.429, 0.756], [['(3,2)'], 'Iris-versicolor', 0.419, 0.75], [['(1,1)', '(3,2)'], 'Iris-versicolor', 0.343, 0.75], [['(1,1)', '(0,2)', '(3,2)'], 'Iris-versicolor', 0.143, 0.733], [['(3,1)', '(0,1)', '(1,1)'], 'Iris-setosa', 0.067, 0.714], [['(0,3)', '(1,1)', '(2,3)', '(3,2)'], 'Iris-virginica', 0.067, 0.714], [['(1,1)', '(2,3)', '(3,2)'], 'Iris-virginica', 0.095, 0.7], [['(0,3)', '(2,3)', '(3,2)'], 'Iris-virginica', 0.095, 0.7], [['(2,3)', '(3,2)'], 'Iris-virginica', 0.124, 0.692], [['(0,3)'], 'Iris-virginica', 0.333, 0.657], [['(1,1)', '(0,2)'], 'Iris-versicolor', 0.19, 0.65], [['(0,3)', '(3,2)'], 'Iris-versicolor', 0.181, 0.632], [['(0,1)', '(1,1)'], 'Iris-versicolor', 0.152, 0.625], [['(0,3)', '(1,1)'], 'Iris-virginica', 0.2, 0.619], [['(0,3)', '(1,1)', '(3,2)'], 'Iris-versicolor', 0.124, 0.615], [['(0,2)'], 'Iris-versicolor', 0.238, 0.6], [['(3,1)', '(1,1)'], 'Iris-setosa', 0.086, 0.556], [['(1,1)'], 'Iris-versicolor', 0.543, 0.544], [['(3,1)', '(0,2)'], 'Iris-versicolor', 0.038, 0.5], [['(3,1)', '(0,2)'], 'Iris-setosa', 0.038, 0.5]]\n",
            "Selected Rules: \n",
            "[['(2,1)'], 'Iris-setosa', 0.343, 1.0]\n",
            "[['(3,3)'], 'Iris-virginica', 0.2, 1.0]\n",
            "[['(0,1)', '(2,2)'], 'Iris-versicolor', 0.095, 1.0]\n",
            "[['(0,3)', '(2,2)'], 'Iris-versicolor', 0.086, 1.0]\n",
            "[['(3,1)', '(2,2)'], 'Iris-versicolor', 0.038, 1.0]\n",
            "[['(2,2)'], 'Iris-versicolor', 0.333, 0.943]\n",
            "[['(0,3)', '(2,3)'], 'Iris-virginica', 0.248, 0.885]\n",
            "default_class: Iris-virginica\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcfUN89FSRcs"
      },
      "source": [
        "### CBA-CB - M2 Improved Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NtbCfRb-P7y"
      },
      "source": [
        "class Classifier_m2:\n",
        "    \"\"\"\n",
        "    The definition of classifier formed in CBA-CB: M2. It contains a list of rules order by their precedence, a default\n",
        "    class label.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.rule_list = list()\n",
        "        self.default_class = None\n",
        "        self._default_class_list = list()\n",
        "        self._total_errors_list = list()\n",
        "\n",
        "    # insert a new rule into classifier\n",
        "    def add(self, rule, default_class, total_errors):\n",
        "        #print(\"Adding\")\n",
        "        self.rule_list.append(rule)\n",
        "        self._default_class_list.append(default_class)\n",
        "        self._total_errors_list.append(total_errors)\n",
        "\n",
        "    # discard those rules that introduce more errors. See line 18-20, CBA-CB: M2 (Stage 3).\n",
        "    def discard(self):\n",
        "        index = self._total_errors_list.index(min(self._total_errors_list))\n",
        "        self.rule_list = self.rule_list[:(index + 1)]\n",
        "        self._total_errors_list = None\n",
        "\n",
        "        self.default_class = self._default_class_list[index]\n",
        "        self._default_class_list = None\n",
        "\n",
        "    # just print out rules and default class label\n",
        "    def print(self):\n",
        "        print(\"Selected Rules: \")\n",
        "        for rule in self.rule_list:\n",
        "            print(rule[:4])\n",
        "        print(\"default_class:\", self.default_class)\n",
        "\n",
        "def return_cases(dataset):\n",
        "        class_column = [x[-1] for x in dataset]\n",
        "        class_label = set(class_column)\n",
        "        x = dict((x, 0) for x in class_label)\n",
        "        #print(x)\n",
        "        return x\n",
        "\n",
        "# convert ruleitem of class RuleItem to rule of class Rule\n",
        "def ruleitem2rule(rule_item, dataset):\n",
        "    rule_item.append(return_cases(dataset))\n",
        "    rule_item.append(set())\n",
        "    #rule = Rule(rule_item.cond_set, rule_item.class_label, dataset)\n",
        "    #print(rule_item)\n",
        "    return rule_item\n",
        "\n",
        "\n",
        "# finds the highest precedence rule that covers the data case d from the set of rules having the same class as d.\n",
        "def maxCoverRule_correct(cars_list, data_case):\n",
        "    for i in range(len(cars_list)):\n",
        "        if cars_list[i][1] == data_case[-1]:\n",
        "            if is_satisfy(data_case, cars_list[i]):\n",
        "                return i\n",
        "    return None\n",
        "\n",
        "\n",
        "# finds the highest precedence rule that covers the data case d from the set of rules having the different class as d.\n",
        "def maxCoverRule_wrong(cars_list, data_case):\n",
        "    for i in range(len(cars_list)):\n",
        "        if cars_list[i][1] != data_case[-1]:\n",
        "            temp_data_case = data_case[:-1]\n",
        "            temp_data_case.append(cars_list[i][1])\n",
        "            if is_satisfy(temp_data_case, cars_list[i]):\n",
        "                return i\n",
        "    return None\n",
        "\n",
        "\n",
        "# compare two rule, return the precedence.\n",
        "#   -1: rule1 < rule2, 0: rule1 < rule2 (randomly here), 1: rule1 > rule2\n",
        "def compare(rule1, rule2):\n",
        "    if rule1 is None and rule2 is not None:\n",
        "        return -1\n",
        "    elif rule1 is None and rule2 is None:\n",
        "        return 0\n",
        "    elif rule1 is not None and rule2 is None:\n",
        "        return 1\n",
        "\n",
        "    if rule1[3] < rule2[3]:     # 1. the confidence of ri > rj\n",
        "        return -1\n",
        "    elif rule1[3] == rule2[3]:\n",
        "        if rule1[2] < rule2[2]:       # 2. their confidences are the same, but support of ri > rj\n",
        "            return -1\n",
        "        elif rule1[2] == rule2[2]:\n",
        "            if len(rule1[0]) < len(rule2[0]):   # 3. confidence & support are the same, ri earlier than rj\n",
        "                return 1\n",
        "            elif len(rule1[0]) == len(rule2[0]):\n",
        "                return 0\n",
        "            else:\n",
        "                return -1\n",
        "        else:\n",
        "            return 1\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "\n",
        "# finds all the rules in U that wrongly classify the data case and have higher precedences than that of its cRule.\n",
        "def allCoverRules(u, data_case, c_rule, cars_list):\n",
        "    w_set = []\n",
        "    for rule_index in u:\n",
        "        # have higher precedences than cRule\n",
        "        if compare(cars_list[rule_index], c_rule) > 0:\n",
        "            # wrongly classify the data case\n",
        "            if is_satisfy(data_case, cars_list[rule_index]) == False:\n",
        "                w_set.append(rule_index)\n",
        "    return w_set\n",
        "\n",
        "\n",
        "# counts the number of training cases in each class\n",
        "def compClassDistr(dataset):\n",
        "    class_distr = dict()\n",
        "\n",
        "    if len(dataset) <= 0:\n",
        "        class_distr = None\n",
        "\n",
        "    dataset_without_null = dataset\n",
        "    while [] in dataset_without_null:\n",
        "        dataset_without_null.remove([])\n",
        "\n",
        "    class_column = [x[-1] for x in dataset_without_null]\n",
        "    class_label = set(class_column)\n",
        "    for c in class_label:\n",
        "        class_distr[c] = class_column.count(c)\n",
        "    return class_distr\n",
        "\n",
        "\n",
        "# sort the rule list order by precedence\n",
        "def sort_with_index(q, cars_list):\n",
        "    def cmp_method(a, b):\n",
        "        # 1. the confidence of ri > rj\n",
        "        if cars_list[a][3] < cars_list[b][3]:\n",
        "            return 1\n",
        "        elif cars_list[a][3] == cars_list[b][3]:\n",
        "            # 2. their confidences are the same, but support of ri > rj\n",
        "            if cars_list[a][2] < cars_list[b][2]:\n",
        "                return 1\n",
        "            elif cars_list[a][2] == cars_list[b][2]:\n",
        "                # 3. both confidence & support are the same, ri earlier than rj\n",
        "                if len(cars_list[a][0]) < len(cars_list[b][0]):\n",
        "                    return -1\n",
        "                elif len(cars_list[a][0]) == len(cars_list[b][0]):\n",
        "                    return 0\n",
        "                else:\n",
        "                    return 1\n",
        "            else:\n",
        "                return -1\n",
        "        else:\n",
        "            return -1\n",
        "\n",
        "    rule_list = q\n",
        "    rule_list.sort(key=functools.cmp_to_key(cmp_method))\n",
        "    return rule_list\n",
        "\n",
        "\n",
        "# get how many errors the rule wrongly classify the data case\n",
        "def errorsOfRule(rule, dataset):\n",
        "    error_number = 0\n",
        "    for case in dataset:\n",
        "        if case:\n",
        "            if is_satisfy(case, rule) == False:\n",
        "                error_number += 1\n",
        "    return error_number\n",
        "\n",
        "\n",
        "# choose the default class (majority class in remaining dataset)\n",
        "def selectDefault(class_distribution):\n",
        "    if class_distribution is None:\n",
        "        return None\n",
        "\n",
        "    max = 0\n",
        "    default_class = None\n",
        "    for index in class_distribution:\n",
        "        if class_distribution[index] > max:\n",
        "            max = class_distribution[index]\n",
        "            default_class = index\n",
        "    return default_class\n",
        "\n",
        "\n",
        "# count the number of errors that the default class will make in the remaining training data\n",
        "def defErr(default_class, class_distribution):\n",
        "    if class_distribution is None:\n",
        "        import sys\n",
        "        return sys.maxsize\n",
        "\n",
        "    error = 0\n",
        "    for index in class_distribution:\n",
        "        if index != default_class:\n",
        "            error += class_distribution[index]\n",
        "    return error\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8Pcx1rOXxLg"
      },
      "source": [
        "#### CBA - M2 main classifier builder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O1kPUYtXlfp"
      },
      "source": [
        "# main method, implement the whole classifier builder\n",
        "def classifier_builder_m2(cars, dataset):\n",
        "    classifier = Classifier_m2()\n",
        "\n",
        "    cars_list = prec_sort(cars)\n",
        "    for i in range(len(cars_list)):\n",
        "        cars_list[i] = ruleitem2rule(cars_list[i], dataset)\n",
        "        #print(cars_list[i])\n",
        "    #for j in range()\n",
        "\n",
        "    # stage 1\n",
        "    q = [] # set\n",
        "    u = [] # set\n",
        "    a = [] # set\n",
        "    mark_set = [] # set\n",
        "    for i in range(len(dataset)):\n",
        "        c_rule_index = maxCoverRule_correct(cars_list, dataset[i])\n",
        "        w_rule_index = maxCoverRule_wrong(cars_list, dataset[i])\n",
        "        if c_rule_index is not None:\n",
        "            u.append(c_rule_index)\n",
        "        if c_rule_index:\n",
        "            cars_list[c_rule_index][4][dataset[i][-1]] += 1\n",
        "        if c_rule_index and w_rule_index:\n",
        "            if compare(cars_list[c_rule_index], cars_list[w_rule_index]) > 0:\n",
        "                q.append(c_rule_index)\n",
        "                mark_set.append(c_rule_index)\n",
        "            else:\n",
        "                a.append((i, dataset[i][-1], c_rule_index, w_rule_index))\n",
        "        elif c_rule_index is None and w_rule_index is not None:\n",
        "            a.append((i, dataset[i][-1], c_rule_index, w_rule_index))\n",
        "\n",
        "    print(\"a: \",a)\n",
        "    print(\"mark_set: \",mark_set)\n",
        "    # stage 2\n",
        "    for entry in a:\n",
        "        lol = entry[3]\n",
        "        if cars_list[lol] in mark_set:\n",
        "            if entry[2] is not None:\n",
        "                cars_list[entry[2]][4][entry[1]] -= 1\n",
        "            cars_list[entry[3]][4][entry[1]] += 1\n",
        "        else:\n",
        "            if entry[2] is not None:\n",
        "                w_set = allCoverRules(u, dataset[entry[0]], cars_list[entry[2]], cars_list)\n",
        "            else:\n",
        "                w_set = allCoverRules(u, dataset[entry[0]], None, cars_list)\n",
        "            for w in w_set:\n",
        "                cars_list[w][5].add((entry[2], entry[0], entry[1]))\n",
        "                cars_list[w][4][entry[1]] += 1\n",
        "            q = list(set().union(q,w_set))\n",
        "            #q |= w_set\n",
        "\n",
        "    # stage 3\n",
        "    rule_errors = 0\n",
        "    q = sort_with_index(q, cars_list)\n",
        "    data_cases_covered = list([False] * len(dataset))\n",
        "    for r_index in q:\n",
        "        if cars_list[r_index][4][cars_list[r_index][1]] != 0:\n",
        "            for entry in cars_list[r_index][5]:\n",
        "                if data_cases_covered[entry[1]]:\n",
        "                    cars_list[r_index][4][entry[2]] -= 1\n",
        "                else:\n",
        "                    if entry[0] is not None:\n",
        "                        cars_list[entry[0]][4][entry[2]] -= 1\n",
        "            for i in range(len(dataset)):\n",
        "                datacase = dataset[i]\n",
        "                if datacase:\n",
        "                    is_satisfy_value = is_satisfy(datacase, cars_list[r_index])\n",
        "                    if is_satisfy_value:\n",
        "                        dataset[i] = []\n",
        "                        data_cases_covered[i] = True\n",
        "            rule_errors += errorsOfRule(cars_list[r_index], dataset)\n",
        "            class_distribution = compClassDistr(dataset)\n",
        "            default_class = selectDefault(class_distribution)\n",
        "            default_errors = defErr(default_class, class_distribution)\n",
        "            total_errors = rule_errors + default_errors\n",
        "            classifier.add(cars_list[r_index], default_class, total_errors)\n",
        "    classifier.discard()\n",
        "\n",
        "    return classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AJg-GCCx6_5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41d4e9d1-0dfc-4bd5-b7bc-2149a5bc248f"
      },
      "source": [
        "classifier_m2 = classifier_builder_m2(cars, df)\n",
        "classifier_m2.print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a:  [(1, 'Iris-versicolor', 94, 80), (35, 'Iris-versicolor', 94, 80), (44, 'Iris-virginica', None, 74), (60, 'Iris-virginica', None, 74), (69, 'Iris-versicolor', 94, 80), (100, 'Iris-versicolor', 92, 81)]\n",
            "mark_set:  [4, 19, 81, 17, 17, 80, 19, 80, 17, 17, 42, 17, 4, 17, 4, 19, 4, 4, 4, 4, 80, 4, 4, 80, 17, 81, 19, 17, 19, 4, 42, 4, 80, 17, 19, 80, 4, 19, 4, 19, 19, 80, 4, 17]\n",
            "Selected Rules: \n",
            "[['(3,3)'], 'Iris-virginica', 0.2, 1.0]\n",
            "[['(0,1)', '(2,2)'], 'Iris-versicolor', 0.095, 1.0]\n",
            "[['(0,3)', '(2,2)'], 'Iris-versicolor', 0.086, 1.0]\n",
            "[['(3,1)', '(2,2)'], 'Iris-versicolor', 0.038, 1.0]\n",
            "[['(2,2)'], 'Iris-versicolor', 0.333, 0.943]\n",
            "[['(0,3)', '(2,3)'], 'Iris-virginica', 0.248, 0.885]\n",
            "default_class: Iris-setosa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US5qQZ0FSzjd"
      },
      "source": [
        "# Accuracy Calculator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aF6W8pFwrCI"
      },
      "source": [
        "# calculates and returns the accuracy of the classifier on the dataset\n",
        "def get_accuracy(classifier, dataset):\n",
        "    size = len(dataset)\n",
        "    correct_match = 0\n",
        "    error_number = 0\n",
        "    for case in dataset:\n",
        "        is_satisfy_value = False\n",
        "        for rule in classifier.rule_list:\n",
        "            is_satisfy_value = is_satisfy(case, rule)\n",
        "            #print(is_satisfy_value)\n",
        "            if is_satisfy_value == True:\n",
        "                correct_match += 1\n",
        "                break\n",
        "        if is_satisfy_value == False:\n",
        "            error_number += 1\n",
        "    return correct_match / (error_number + correct_match)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4DnI5D5YBJ0"
      },
      "source": [
        "### 10 - Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywmMRS8gYGOH"
      },
      "source": [
        "block_size = int(len(test_df) / 10)\n",
        "split_point = [k * block_size for k in range(0, 10)]\n",
        "split_point.append(len(test_df))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD9vyeoiSE9X"
      },
      "source": [
        "For M1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiLcBTFe__3P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14991b46-d012-44b0-e1fe-1f9c99fca4b0"
      },
      "source": [
        "for k in range(len(split_point)-1):\n",
        "    testt_df = d[split_point[k]:split_point[k+1]]\n",
        "    print(\"Accuracy\",k+1,\": \",get_accuracy(classifier_m1,testt_df))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy 1 :  1.0\n",
            "Accuracy 2 :  1.0\n",
            "Accuracy 3 :  1.0\n",
            "Accuracy 4 :  1.0\n",
            "Accuracy 5 :  1.0\n",
            "Accuracy 6 :  1.0\n",
            "Accuracy 7 :  1.0\n",
            "Accuracy 8 :  1.0\n",
            "Accuracy 9 :  1.0\n",
            "Accuracy 10 :  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgHI5zcZvZp2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69964a49-57fc-46c8-f41c-f541772ea567"
      },
      "source": [
        "# accuracy of M1\n",
        "accuracy_m1 = get_accuracy(classifier_m1,d)\n",
        "print(\"Accuracy for M1: \",accuracy_m1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for M1:  0.9787234042553191\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dX0YFBNZ3yRJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7f73b75-38ce-41fc-a8e8-d7e64e75d285"
      },
      "source": [
        "# accuracy of M2\n",
        "accuracy_m2 = get_accuracy(classifier_m2,d)\n",
        "print(\"Accuracy for M2: \",accuracy_m2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for M2:  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LggR74jZxak"
      },
      "source": [
        "d = test_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRTNC6qSzUY-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}